{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC MODELLING \n",
    "\n",
    "\n",
    "### 1. Latent Dirichlet Analysis (LDA)\n",
    "\n",
    "The idea is to first <i>LEARN</i> the topics from the Titles fields. This requires us to concatenate all the Titles from the cleaned datatables. \n",
    "\n",
    "In order to achieve this we have to follow these steps: \n",
    "\n",
    "   1. Create a corpus from the tokenised Titles\n",
    "   2. Create a Dictionary of (word-id, word) pairs \n",
    "   3. Vectorise the corpus using a simple Bag of Words (BOW) // we could also use bi or tri grams\n",
    "   4. Train an LDA model on the vectorised corpus\n",
    "   \n",
    "At the end, we shall have an LDA model that predicts the topic distribution for each document (probability distribution), as well as the word distribution per topic.\n",
    "\n",
    "We shall use the <b> gensim library </b> to do the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Corpus Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py35/lib/python3.5/site-packages/gensim-0.13.3-py3.5-macosx-10.6-x86_64.egg/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from ast import literal_eval\n",
    "from gensim.models import LdaModel\n",
    "import random , pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost we require a corpus of documents which are the titles in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/arraysetops.py:395: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "title_corpus = []\n",
    "\n",
    "for filename in ['dblp_books_clean.csv','dblp_articles_clean.csv' , 'dblp_incollections_clean.csv',\n",
    "                 'dblp_inproceedings_clean.csv','dblp_proceedings_clean.csv', 'dblp_theses_clean.csv'\n",
    "                ]:\n",
    "    vals = pd.read_csv(filename).Title.unique()\n",
    "    vals = [literal_eval(val) for val in vals]\n",
    "    title_corpus.extend(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dictionary Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_dict = Dictionary(title_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333919"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(vocab_dict, open( \"full_vocab.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_dict.filter_extremes(no_above=0.9, no_below=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50806"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(vocab_dict, open( \"cut_vocab.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 BOW Vectorisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_corpus = [vocab_dict.doc2bow(x) for x in title_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3748124"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Sampling & LDA Model \n",
    "\n",
    "We would need to sample our dataset as 3 million samples would take forever to calculate on a single computer. \n",
    "We sample 100,000 documents at random without replacement and train an LDA model on it. Note the number of topics required would need to be <b>CROSS VALIDATED BASED ON THE RECOMMENDATION QUALITY</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = random.sample(range(len(bow_corpus)), 100000)\n",
    "bow_sample =[bow_corpus[x] for x in sorted(sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 24s, sys: 3.13 s, total: 3min 27s\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%time lda_model = LdaModel( corpus = bow_sample,id2word=vocab_dict,num_topics=10, passes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(lda_model, open('lda_model_10_topics','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.027*\"based\" + 0.023*\"image\" + 0.020*\"icassp\" + 0.018*\"recognition\" + 0.015*\"detection\" + 0.014*\"processing\" + 0.012*\"video\" + 0.010*\"images\" + 0.010*\"speech\" + 0.009*\"estimation\"'),\n",
       " (1,\n",
       "  '0.022*\"logic\" + 0.011*\"complexity\" + 0.010*\"graphs\" + 0.010*\"graph\" + 0.009*\"theory\" + 0.009*\"set\" + 0.008*\"program\" + 0.008*\"order\" + 0.008*\"sets\" + 0.007*\"matrix\"'),\n",
       " (2,\n",
       "  '0.032*\"based\" + 0.026*\"time\" + 0.023*\"systems\" + 0.013*\"real\" + 0.013*\"fuzzy\" + 0.013*\"control\" + 0.013*\"multi\" + 0.012*\"model\" + 0.010*\"analysis\" + 0.009*\"selection\"'),\n",
       " (3,\n",
       "  '0.027*\"proceedings\" + 0.018*\"learning\" + 0.015*\"systems\" + 0.014*\"study\" + 0.013*\"symposium\" + 0.012*\"case\" + 0.012*\"information\" + 0.010*\"zur\" + 0.010*\"intelligent\" + 0.009*\"robot\"'),\n",
       " (4,\n",
       "  '0.011*\"design\" + 0.009*\"high\" + 0.009*\"simulation\" + 0.008*\"localization\" + 0.007*\"analysis\" + 0.007*\"functional\" + 0.007*\"synthesis\" + 0.007*\"experimental\" + 0.006*\"eines\" + 0.005*\"speed\"'),\n",
       " (5,\n",
       "  '0.023*\"optimization\" + 0.015*\"algorithm\" + 0.012*\"problem\" + 0.011*\"problems\" + 0.010*\"method\" + 0.009*\"algorithms\" + 0.008*\"multi\" + 0.007*\"revised\" + 0.007*\"optimal\" + 0.007*\"parallel\"'),\n",
       " (6,\n",
       "  '0.022*\"web\" + 0.021*\"conference\" + 0.021*\"international\" + 0.020*\"based\" + 0.017*\"data\" + 0.012*\"automatic\" + 0.010*\"semantic\" + 0.010*\"ijcai\" + 0.009*\"mining\" + 0.008*\"approach\"'),\n",
       " (7,\n",
       "  '0.022*\"und\" + 0.019*\"von\" + 0.018*\"der\" + 0.017*\"des\" + 0.013*\"f√ºr\" + 0.011*\"et\" + 0.008*\"pour\" + 0.007*\"compression\" + 0.007*\"information\" + 0.007*\"integration\"'),\n",
       " (8,\n",
       "  '0.048*\"networks\" + 0.022*\"workshops\" + 0.020*\"workshop\" + 0.019*\"mobile\" + 0.018*\"wireless\" + 0.017*\"network\" + 0.015*\"based\" + 0.015*\"sensor\" + 0.012*\"distributed\" + 0.010*\"communication\"'),\n",
       " (9,\n",
       "  '0.022*\"software\" + 0.020*\"th\" + 0.015*\"systems\" + 0.014*\"based\" + 0.012*\"design\" + 0.011*\"data\" + 0.011*\"conference\" + 0.011*\"service\" + 0.010*\"architecture\" + 0.010*\"oriented\"')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see from the above that there still is ertain overlap between the topics but these resuls are good enough for the purpose of our prototype. \n",
    "\n",
    "#### It can also be seen that the stopwords in french and german are posing a problem which need to be removed during the cleaning.\n",
    "This happend because we only removed English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
